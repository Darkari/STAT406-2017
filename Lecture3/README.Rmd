---
title: "STAT406 - Lecture 3 notes"
author: "Matias Salibian-Barrera"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Lecture slides

The lecture slides are [here](STAT406-17-lecture-3.pdf).

## Cross-validation when the model is chosen using the data

In this document we study how to perform cross-validation 
when the model was selected or determined using the 
training data. Consider the following synthetic data 
set
```{r load.fallacy}
dat <- read.table('fallacy.dat', header=TRUE, sep=',')
```
This is the same data used in class. In this example
we know what is the "true" model, and thus we also know
what is the "optimal" predictor we should 
train using this data. 

However, we now decide to build a good
linear model using forward stepwise (AIC-based):
```{r fallacy}
library(MASS)
p <- ncol(dat)
null <- lm(Y~1, data=dat)
full <- lm(Y~., data=dat) # needed for stepwise
step.lm <- stepAIC(null, scope=list(lower=null, upper=full), trace=FALSE)
```
Without thinking, we use CV to compare the MSPE of the 
**null** model (which we know is "true") and the
one we obtained using forward stepwise:
```{r wrong}
n <- nrow(dat)
ii <- (1:n) %% 5 + 1
set.seed(17)
N <- 10
mspe.n <- mspe.st <- rep(0, N)
for(i in 1:N) {
  ii <- sample(ii)
  pr.n <- pr.st <- rep(0, n)
  for(j in 1:5) {
    tmp.st <- update(step.lm, data=dat[ii != j, ])
    pr.st[ ii == j ] <- predict(tmp.st, newdata=dat[ii == j, ])
    pr.n[ ii == j ] <- with(dat[ii != j, ], mean(Y))
  }
  mspe.st[i] <- with(dat, mean( (Y - pr.st)^2 ))
  mspe.n[i] <- with(dat, mean( (Y - pr.n)^2 ))
}
boxplot(mspe.st, mspe.n, names=c('Stepwise', 'NULL'), col=c('gray60', 'hotpink'), main='Wrong')
summary(mspe.st)
summary(mspe.n)
```

* **Something is wrong!** Why? 
* What would you change above to obtain reliable estimates for the MSPE of the 
model selected with the stepwise approach? 


## Correlated covariates

Variables that are significant in a smaller model,
"dissappear" in a larger one
```{r signif}
# Correlated covariates
x <- read.table('../Lecture1/rutgers-lib-30861_CSV-1.csv', header=TRUE, sep=',')
reduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data=x)
full <- lm(MORT ~ ., data=x)
# significant variables in "reduced" dissappear in "full"
round( summary(reduced)$coef, 3)
round( summary(full)$coef[ names(coef(reduced)), ], 3)
```
