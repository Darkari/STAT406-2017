---
title: "STAT406 - Lecture 3 notes"
author: "Matias Salibian-Barrera"
date: "`r format(Sys.Date())`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Lecture slides

The lecture slides are [here](STAT406-17-lecture-3.pdf).

## Cross-validation

Load the data
```{r load, fig.width=5, fig.height=5, echo=TRUE}
data <- read.csv('../Lecture1/rutgers-lib-30861_CSV-1.csv')
m.f <- lm(MORT ~ . , data=data)
m.r <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data=data)
```

We use 10 runs of 5-fold CV comparing the full and reduced models.
Again, here we assume that the reduced model was not obtained
using the training data. 
```{r cv10runs, fig.width=5, fig.height=5, echo=TRUE}
N <- 100
mspe1 <- mspe2 <- vector('double', N)
ii <- (1:(n <- nrow(data))) %% 5 + 1
set.seed(327)
for(i in 1:N) {
  ii <- sample(ii)
  pr.f <- pr.r <- vector('double', n)
  for(j in 1:5) {
    pr.f[ ii == j ] <- predict(update(m.f, data=data[ii != j, ]), newdata=data[ii==j,])
    pr.r[ ii == j ] <- predict(update(m.r, data=data[ii != j, ]), newdata=data[ii==j,])
  }
  mspe1[i] <- with(data, mean( (MORT - pr.f)^2 ))
  mspe2[i] <- with(data, mean( (MORT - pr.r)^2 ))
}  
boxplot(mspe1, mspe2, names=c('Full', 'Reduced'), 
        col=c('gray80', 'tomato3'), 
        main='Air Pollution - 10 runs 5-fold CV')
mtext(expression(hat(MSPE)), side=2, line=2.5)
```

## What happens if the model is chosen using the training data?

Use forward stepwise (with AIC) on this synthetic data
```{r fallacy}
library(MASS)
dat <- read.table('fallacy.dat', header=TRUE, sep=',')
n <- nrow(dat)
p <- ncol(dat)
null <- lm(Y~1, data=dat)
full <- lm(Y~., data=dat) # needed for stepwise
step.lm <- stepAIC(null, scope=list(lower=null, upper=full), trace=FALSE)
```

We now attempt to use CV to compare the MSPE of the 
*null* model (which we know is true) and the
one based on stepwise:
```{r wrong}
ii <- (1:n) %% 5 + 1
set.seed(17)
N <- 10
mspe.n <- mspe.st <- rep(0, N)
for(i in 1:N) {
  ii <- sample(ii)
  pr.n <- pr.st <- rep(0, n)
  for(j in 1:5) {
    tmp.st <- update(step.lm, data=dat[ii != j, ])
    pr.st[ ii == j ] <- predict(tmp.st, newdata=dat[ii == j, ])
    pr.n[ ii == j ] <- with(dat[ii != j, ], mean(Y)) #mean((dat$Y)[ii != j])
  }
  mspe.st[i] <- with(dat, mean( (Y - pr.st)^2 ))
  mspe.n[i] <- with(dat, mean( (Y - pr.n)^2 ))
}
boxplot(mspe.st, mspe.n, names=c('Stepwise', 'NULL'), col=c('gray60', 'hotpink'), main='Wrong')
summary(mspe.st)
summary(mspe.n)
```

**Something is wrong! **






## Correlated covariates

Variables that are significant in a smaller model,
"dissappear" in a larger one
```{r signif}
# Correlated covariates
x <- read.table('../Lecture1/rutgers-lib-30861_CSV-1.csv', header=TRUE, sep=',')
reduced <- lm(MORT ~ POOR + HC + NOX + HOUS + NONW, data=x)
full <- lm(MORT ~ ., data=x)
# significant variables in "reduced" dissappear in "full"
round( summary(reduced)$coef, 3)
round( summary(full)$coef[ names(coef(reduced)), ], 3)
```
